{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Shishir_nlp_hw4_task6.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0hVWeHqafNT","executionInfo":{"status":"ok","timestamp":1638646994851,"user_tz":360,"elapsed":20059,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"4c2eb9b6-4052-4c39-8161-286e29b24a03"},"source":["!pip install -U spacy\n","!pip install -U gensim"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Collecting spacy\n","  Downloading spacy-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 50.3 MB/s \n","\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n","  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n","\u001b[K     |████████████████████████████████| 628 kB 56.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Collecting spacy-legacy<3.1.0,>=3.0.8\n","  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n","Collecting pathy>=0.3.5\n","  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 959 kB/s \n","\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 65.6 MB/s \n","\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n","  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n","Collecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n","Collecting srsly<3.0.0,>=2.4.1\n","  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 49.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Collecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.0 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Collecting gensim\n","  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n","\u001b[K     |████████████████████████████████| 24.1 MB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed gensim-4.1.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"nwMq-Jgk3_2t"},"source":["# **Import libraries**"]},{"cell_type":"code","metadata":{"id":"d2nDNdsU_a1Z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqYqOtp5yluv"},"source":["# Importing the necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR, StepLR\n","\n","from torchtext.vocab import Vocab, vocab\n","from torchtext.vocab import build_vocab_from_iterator\n","import joblib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.io as pio\n","pio.renderers.default = 'colab'\n","\n","import numpy as np\n","import pandas as pd\n","import re\n","from bs4 import BeautifulSoup\n","import random\n","from datetime import datetime\n","from pathlib import Path\n","from scipy.sparse import hstack\n","from collections import Counter, OrderedDict\n","\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import spacy\n","from spacy.matcher import Matcher\n","from spacy.tokens import Token\n","\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","import gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4OpOEo0QktF"},"source":["# Install wandb and update it to the latest version\n","%%capture\n","!pip install wandb --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"faY9UaHhQkb3","executionInfo":{"status":"ok","timestamp":1638647014936,"user_tz":360,"elapsed":3008,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"9d0088fe-0790-43e8-9fb6-afbeb37ec085"},"source":["# Import wandb\n","import wandb\n","\n","# Login to W&B\n","wandb.login()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"OfGrJujc4G6S"},"source":["## **Import Dataset csv file**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_4UPFbXFX8l","executionInfo":{"status":"ok","timestamp":1638647033129,"user_tz":360,"elapsed":18198,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"b03c0416-1584-466f-f2b3-7ebb98274f9a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"ThYcWeURFC-k"},"source":["# specify the folder in googe drive where we will save dataset\n","basepath = '/content/drive/MyDrive/Data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"omgjHGvS2VBQ"},"source":["data_folder = Path('/content/drive/MyDrive/NLP/Homework21')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAhUOEiZ4ENA"},"source":["spacy_folder = Path('/content/drive/MyDrive/Data/spacy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27OJT5oq2WXI"},"source":["lecture_folder = Path('/content/drive/MyDrive/NLP')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AfxXCoF44R5q"},"source":["save_model_folder = lecture_folder /'Homework21'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXc7p09ebkyA"},"source":["#url = 'https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0.tar.gz'\n","#!wget {url} -P {spacy_folder} "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QoIoh4fDcRg2"},"source":["#file = spacy_folder / 'en_core_web_lg-3.2.0.tar.gz'\n","#with  tarfile.open(file, 'r') as tar:\n","#  tar.extractall(path = spacy_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aHgXiPrJrQQ"},"source":["model = spacy_folder /'en_core_web_lg-3.2.0'/'en_core_web_lg'/'en_core_web_lg-3.2.0'\n","nlp = spacy.load(model, disable=['ner, parser'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"or0BG4cyFooq","executionInfo":{"status":"ok","timestamp":1638647053729,"user_tz":360,"elapsed":16,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"2a2ad2b7-c37d-4528-f48f-bc0e8fd11c85"},"source":["folder = Path(basepath)\n","folder"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PosixPath('/content/drive/MyDrive/Data')"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"i2ce7iBfaaIr","executionInfo":{"status":"ok","timestamp":1638647055554,"user_tz":360,"elapsed":1834,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"e54b9282-cfd1-4627-9047-eb51a3bc883b"},"source":["df = pd.read_csv(folder / 'multilabel_hw.csv')\n","df.drop(columns= ['Unnamed: 0','Unnamed: 0.1','Id'], axis=1, inplace=True)\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>Tags</th>\n","      <th>Tag_Number</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ASP Query String From DropDown</td>\n","      <td>&lt;p&gt;I have a webpage: &lt;strong&gt;Menu.aspx&lt;/strong...</td>\n","      <td>c# asp.net</td>\n","      <td>[0, 9]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>How can I run JavaScript code at server side J...</td>\n","      <td>&lt;p&gt;I want to run JavaScript code at the server...</td>\n","      <td>java javascript</td>\n","      <td>[1, 3]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>linq to sql throwing an exception row not foun...</td>\n","      <td>&lt;p&gt;Hi I am linq to sql and i am getting the er...</td>\n","      <td>c# asp.net</td>\n","      <td>[0, 9]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Running a Python script on a PHP server</td>\n","      <td>&lt;p&gt;I am running a nginx web server, along with...</td>\n","      <td>php python</td>\n","      <td>[2, 7]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>some advice on how to write a window.resize fu...</td>\n","      <td>&lt;p&gt;Im trying to write a function that resizes ...</td>\n","      <td>javascript jquery</td>\n","      <td>[3, 5]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>47422</th>\n","      <td>Taking value of edittext and putting a decimal...</td>\n","      <td>&lt;p&gt;All-\\nI am working on an app in which the u...</td>\n","      <td>java android</td>\n","      <td>[1, 4]</td>\n","    </tr>\n","    <tr>\n","      <th>47423</th>\n","      <td>Listen to phone state throughout the application?</td>\n","      <td>&lt;p&gt;I nee to liste to phone state using phone s...</td>\n","      <td>java android</td>\n","      <td>[1, 4]</td>\n","    </tr>\n","    <tr>\n","      <th>47424</th>\n","      <td>Android UI Thread</td>\n","      <td>&lt;p&gt;i am using threads to do few tasks. and aft...</td>\n","      <td>java android</td>\n","      <td>[1, 4]</td>\n","    </tr>\n","    <tr>\n","      <th>47425</th>\n","      <td>dynamic table row creation in html/Javascript</td>\n","      <td>&lt;p&gt;I have html table with 1 row to fill in job...</td>\n","      <td>asp.net javascript</td>\n","      <td>[9, 3]</td>\n","    </tr>\n","    <tr>\n","      <th>47426</th>\n","      <td>image preloader not working in IE</td>\n","      <td>&lt;p&gt;I have the following code to preload images...</td>\n","      <td>javascript jquery</td>\n","      <td>[3, 5]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>47427 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                   Title  ... Tag_Number\n","0                         ASP Query String From DropDown  ...     [0, 9]\n","1      How can I run JavaScript code at server side J...  ...     [1, 3]\n","2      linq to sql throwing an exception row not foun...  ...     [0, 9]\n","3                Running a Python script on a PHP server  ...     [2, 7]\n","4      some advice on how to write a window.resize fu...  ...     [3, 5]\n","...                                                  ...  ...        ...\n","47422  Taking value of edittext and putting a decimal...  ...     [1, 4]\n","47423  Listen to phone state throughout the application?  ...     [1, 4]\n","47424                                  Android UI Thread  ...     [1, 4]\n","47425      dynamic table row creation in html/Javascript  ...     [9, 3]\n","47426                  image preloader not working in IE  ...     [3, 5]\n","\n","[47427 rows x 4 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hb_l10UU1lx","executionInfo":{"status":"ok","timestamp":1638647055554,"user_tz":360,"elapsed":10,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"118c935b-6f7b-4b74-ee29-dd916a674af9"},"source":["mlb = MultiLabelBinarizer()\n","df1 = pd.DataFrame(mlb.fit_transform(df['Tag_Number']),columns=mlb.classes_)\n","cols = [0,1,12,13]\n","df1.drop(df1.columns[cols],axis=1,inplace=True)\n","print (df1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["       0  1  2  3  4  5  6  7  8  9\n","0      1  0  0  0  0  0  0  0  0  1\n","1      0  1  0  1  0  0  0  0  0  0\n","2      1  0  0  0  0  0  0  0  0  1\n","3      0  0  1  0  0  0  0  1  0  0\n","4      0  0  0  1  0  1  0  0  0  0\n","...   .. .. .. .. .. .. .. .. .. ..\n","47422  0  1  0  0  1  0  0  0  0  0\n","47423  0  1  0  0  1  0  0  0  0  0\n","47424  0  1  0  0  1  0  0  0  0  0\n","47425  0  0  0  1  0  0  0  0  0  1\n","47426  0  0  0  1  0  1  0  0  0  0\n","\n","[47427 rows x 10 columns]\n"]}]},{"cell_type":"code","metadata":{"id":"-LkojGGgU1lz"},"source":["df = pd.concat([df,df1],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNtQ9uSebASj"},"source":["X = df['Body'][10000:20000].values #drop(columns = ['Body','Tag','Tag_Number'])\n","\n","y = df[['0','1','2','3','4','5','6','7','8','9']][10000:20000].values.astype(float)  #'0','1','2','3','4','5','6','7','8','9'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7swN3XQ52IUd"},"source":["# Create train/test/valid\n","splits from dataset. 20000 for train, 20000 for test, remaining for validation dataset"]},{"cell_type":"code","metadata":{"id":"vUcdTMqkvO5P"},"source":["X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.6,random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0-GzhWB0z-Q"},"source":["X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.2,random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ggc3BuAc1FMY","executionInfo":{"status":"ok","timestamp":1638647055856,"user_tz":360,"elapsed":5,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"9c3bf308-fa06-46b6-c437-5d5f1eca776b"},"source":["print(X_train.shape), print(y_train.shape)\n","print(X_test.shape), print(y_test.shape)\n","print(X_valid.shape), print(y_valid.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(6000,)\n","(6000, 10)\n","(800,)\n","(800, 10)\n","(3200,)\n","(3200, 10)\n"]},{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"3lq1EEX2nc34"},"source":["## PreProcess Data"]},{"cell_type":"code","metadata":{"id":"u7k41-PhPkSN"},"source":["class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n","    np.random.seed(0)\n","    def __init__(self, lammetize=True, lower=True, remove_stop=True, remove_punct=True, remove_num=False):\n","        self.remove_stop = remove_stop\n","        self.remove_punct = remove_punct\n","        self.remove_num = remove_num\n","        self. lammetize = lammetize\n","        self.lower = lower\n","\n","    # helpfer functions for basic cleaning \n","    def basic_clean(self,text):\n","        return [re.sub(r'[\\n\\r]',' ',sentence) for sentence in text]\n","\n","    # helper function for pre-processing with spacy\n","    def spacy_preprocessor(self,texts): \n","        nlp=spacy.load(model, disable=['parser','ner'])\n","        ## Add @ as a prefix so that we can separate the word from its token\n","        ## Since we are using pretrained vectors - @ mentions will be different in the pre-trained vocab\n","        \n","        prefixes = list(nlp.Defaults.prefixes)\n","        prefixes += ['@']\n","        prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n","        nlp.tokenizer.prefix_search = prefix_regex.search\n","     \n","        matcher = Matcher(nlp.vocab)\n","        if self.remove_stop:\n","            matcher.add(\"stop_words\", [[{\"is_stop\" : True}]])\n","        if self.remove_punct:\n","            matcher.add(\"punctuation\",[ [{\"is_punct\": True}]])\n","        if self.remove_num:\n","            matcher.add(\"numbers\", [[{\"like_num\": True}]])\n","        Token.set_extension('is_remove', default=False,force=True)\n","        cleaned_text=[]\n","\n","        for doc in nlp.pipe(texts,batch_size=64,disable=['parser','ner']):\n","            matches = matcher(doc)\n","            for _, start, end in matches:\n","                for token in doc[start:end]:\n","                    token._.is_remove =True\n","                    \n","            if self.lammetize:\n","                text = ' '.join(token.lemma_ for token in doc if (token._.is_remove==False))\n","            else:\n","                text = ' '.join(token.text for token in doc if (token._.is_remove==False))\n","            if self.lower:\n","                text=text.lower()\n","            cleaned_text.append(text)\n","        return cleaned_text\n","\n","    def fit(self, X,y=None):\n","        return self\n","\n","    def transform(self, X,y=None):\n","        x_clean = self.basic_clean(X)\n","        x_clean_final = self.spacy_preprocessor(x_clean)\n","        return x_clean_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2a1-cHMOaVmP"},"source":["#preprocessor = SpacyPreprocessor()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGlHhdciaVmT"},"source":["#X_train_cleaned = preprocessor.fit_transform(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMKEErcUaVmT"},"source":["#X_valid_cleaned = preprocessor.transform(X_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5eZ9WkbaVmT"},"source":["#X_test_cleaned = preprocessor.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ox6EAaCPaVmT"},"source":["#X_train_cleaned = np.array(X_train_cleaned)\n","#X_valid_cleaned = np.array(X_valid_cleaned)\n","#X_test_cleaned = np.array(X_test_cleaned)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swFQDyWhpJNL"},"source":["file_X_train_cleaned_data = data_folder/ 'X_train_multiclass_clean_task5.pkl'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8pWVodNrMTw"},"source":["file_X_valid_cleaned_data = data_folder/ 'X_valid_multiclass_clean_task5.pkl'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMOnrfj3rM1g"},"source":["file_X_test_cleaned_data = data_folder/'X_test_multiclass_clean_task.pkl'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TwLjLsNyYTFh"},"source":["#joblib.dump(X_train_cleaned, file_X_train_cleaned_data) \n","#joblib.dump(X_valid_cleaned, file_X_valid_cleaned_data) \n","#joblib.dump(X_test_cleaned, file_X_test_cleaned_data) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AP60Md9ysFVH"},"source":["X_train_cleaned = joblib.load(file_X_train_cleaned_data)\n","X_valid_cleaned = joblib.load(file_X_valid_cleaned_data)\n","X_test_cleaned = joblib.load(file_X_test_cleaned_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BPRCikGoe9t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647070783,"user_tz":360,"elapsed":39,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"37d00965-9443-4884-e29d-df3a500babba"},"source":["print(X_train_cleaned.shape)\n","print(X_test_cleaned.shape)\n","print(X_valid_cleaned.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(6000,)\n","(800,)\n","(3200,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"rsYaM3_ftTgi"},"source":["## Custom Dataset Class"]},{"cell_type":"code","metadata":{"id":"xx0-0QP48kCH"},"source":["class CustomDataset(torch.utils.data.Dataset):\n","    \"\"\"IMDB dataset.\"\"\"\n","\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        text = self.X[idx]\n","        text = np.array(text)\n","        labels = self.y[idx]\n","        labels = np.array(labels)\n","        sample = (labels,text)\n","        \n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDesWA-Ssmk_"},"source":["trainset = CustomDataset(X_train_cleaned,y_train)\n","validset = CustomDataset(X_valid_cleaned,y_valid)\n","testset = CustomDataset(X_test_cleaned,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8C7RxRg0TCcO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647070784,"user_tz":360,"elapsed":15,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"9684eeb1-e2ba-4d8b-b1d3-80b8a699069c"},"source":["trainset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.CustomDataset at 0x7fcb3d8fcf90>"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"74B3jEnYvnDj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647070784,"user_tz":360,"elapsed":13,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"1dfe0f57-1095-4e7a-8161-f3cc5989fcc4"},"source":["trainset.__getitem__([0,2])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n"," array(['< p > way embed user control windows explorer tell resource < /p >   < p > thanks weipeng</p >',\n","        '< p > need help copde datum table manually create row column dt populate lastly bind gridview.</p >   < p > want add cell datarow datum table cell turn hold html control html anchor tag < /p >   < p > current dt 2rows 3 col below</p >   < pre><code > server   blah   blah abc      xyz    123 def      vbh    345 < /code></pre >   < p > want processing servername col1 add extra col dt hold html anchor tag detail see click html anchor tag ultimately dt look like below:</p >   < pre><code > server   blah   blah   abc      xyz    123    html link def      vbh    345    html link < /code></pre >   < p > tell i.e add separate cell dt add html control bewly add cell.</p >   < p > thanks rahul</p >'],\n","       dtype='<U32849'))"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"UXQfgcU8wbOw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r58Ci_SB1H2_"},"source":["## Small Sample"]},{"cell_type":"code","metadata":{"id":"JR3FVcmszl1R"},"source":["# We will be using only 100 images for nboth train and validation datasets\n","train_sample_size = int(len(trainset)/10)\n","valid_sample_size = int(len(validset)/7)\n","\n","# Getting n random indices\n","train_subset_indices = random.sample(range(0, len(trainset)), train_sample_size)\n","valid_subset_indices = random.sample(range(0, len(validset)), valid_sample_size)\n","\n","# Getting subset of dataset\n","train_subset = torch.utils.data.Subset(trainset, train_subset_indices)\n","valid_subset = torch.utils.data.Subset(validset, valid_subset_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTM7CV-B5NMs"},"source":["# craete a train and test dataloader\n","#train_subset = CustomDataset(X_train_cleaned[train_subset_indices],y_train[train_subset_indices])\n","                          \n","#valid_subset = CustomDataset(X_valid_cleaned[valid_subset_indices],y_valid[valid_subset_indices])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hqSJVFmxLrV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XoEVe7eGtY7U"},"source":["## Create Vocab"]},{"cell_type":"code","metadata":{"id":"LRLHO-M1to6q"},"source":["counter = Counter()\n","for (label, line) in trainset:\n","   counter.update(str(line).split())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNUi_aNctkAo"},"source":["#Sorting the words based on their frequency and creating OrderedDict from it in descending order\n","sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n","ordered_dict = OrderedDict(sorted_by_freq_tuples)\n","vocab_dict = vocab(counter, min_freq=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyaeFqCsuRTQ"},"source":["#vocab_dict.get_stoi()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7F6-PP4buezX","executionInfo":{"status":"ok","timestamp":1638647071816,"user_tz":360,"elapsed":12,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"b9672c0a-e272-462a-d235-934105571c7b"},"source":["len(vocab_dict)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25039"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"WDMu8-JOujxY"},"source":["vocab_dict.insert_token('<unk>', 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pe1rtp_YuxWr"},"source":["vocab_dict.set_default_index(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mV3jjGz5yNJO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojvSMWR5u-LU"},"source":["## Collate_fn for Data Loaders"]},{"cell_type":"code","metadata":{"id":"cdBKJlVmvB-9"},"source":["# Creating a lambda function objects that will be used to get the indices of words from vocab\n","text_pipeline = lambda x: [vocab_dict[token] for token in str(x).split()]\n","label_pipeline = lambda x: [int(i) for i in x]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cL1TZVthvIT2"},"source":["'''\n","We know that input to the embedding layers are indices of words from the vocab.\n","The collate_batch() accepts batch of data and gets the indices of text from vocab and returns the same\n","We will include this collate_batch() in collat_fn attribute of DataLoader.\n","So it will create a batch of data containing indices of words and corresponding labels.\n","But for EmbeddingBag we need one more extra parameter, that is offset.\n","offsets determines the starting index position of each bag (sequence) in input.\n","'''\n","def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for (_label, _text) in batch:\n","         label_list.append(label_pipeline(_label))\n","         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","         text_list.append(processed_text)\n","         offsets.append(processed_text.size(0))\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text_list = torch.cat(text_list)\n","    return text_list, label_list,  offsets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFnec6uLyhr3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6iHHm8OGEDBR"},"source":["## **Check Data Loader**\n","\n"]},{"cell_type":"code","metadata":{"id":"4vxxfJKvECDo"},"source":["batch_size=2\n","check_loader= torch.utils.data.DataLoader(dataset=trainset,\n","                                        batch_size=batch_size,\n","                                        shuffle=True,\n","                                        collate_fn=collate_batch,\n","                                        num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkWdSP5q5WHd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hc49Kv46HMqJ","executionInfo":{"status":"ok","timestamp":1638647072024,"user_tz":360,"elapsed":214,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"66942019-011c-48bc-ed15-3cef8cf61995"},"source":["for label, text, offsets in check_loader:\n","  print(label, text, offsets)\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([    1,   259,     3,     1,   260,     3,   261,   262,     3,     1,\n","          263,     0,   409,   140,   678,   745,   756,   840,     0,   409,\n","          140,   678,   745,     0,   840,     0,     3,     1,    12,     3,\n","            1,   268,     3,     1,     2,     3,   269,   646,   140,    39,\n","         2760,  2161,   255,   399,   109,    42,  4471,   289,   140,   678,\n","         3261,   251,   678,  2092,   745,   544,  1105,    39,  1096,   616,\n","          824,   140,  6034,   680,   164,  1041,   589,   140,  1063,   329,\n","          473,   678,   745,   255,   586,   140,    34,    35,   745,    28,\n","          778,   678,  1956,   131,   327,  2795,  1508, 14554,     3,     1,\n","          578,   263,     0,   255,  4918,  2092,   136,     0,   255,  4918,\n","         2092,   136,     0,     3,     1,     2,     3,   329,  2510,     3,\n","            1,    68,     3,     0,     1,    40,     3,     1,     2,     3,\n","          109,  1950,   251, 13653,     3,     1,     2,     3,  3403,   891,\n","         4799,     3,     1,   259,     3,     1,     2,     3,   678,  1301,\n","          552,   306,     0,     3,     1,   268,     3,     1,     2,     3,\n","          876,   707,     4,  1239,     3]) tensor([[0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n","        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]) tensor([  0, 104])\n"]}]},{"cell_type":"markdown","metadata":{"id":"RiByneM3JxnV"},"source":["# Create Weight Matrix of Pretrained Weights"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:45:58.138129Z","iopub.status.busy":"2021-06-30T02:45:58.138017Z","iopub.status.idle":"2021-06-30T02:45:58.318850Z","shell.execute_reply":"2021-06-30T02:45:58.318429Z","shell.execute_reply.started":"2021-06-30T02:45:58.138114Z"},"tags":[],"id":"J2XXdgCgWNlU"},"source":["pretrained_vectors = KeyedVectors.load('/content/drive/MyDrive/NLP/Homework21/model_df.bin')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:45:58.319424Z","iopub.status.busy":"2021-06-30T02:45:58.319323Z","iopub.status.idle":"2021-06-30T02:45:58.377964Z","shell.execute_reply":"2021-06-30T02:45:58.377626Z","shell.execute_reply.started":"2021-06-30T02:45:58.319411Z"},"id":"yjy2vqMzJxnV","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647091362,"user_tz":360,"elapsed":20,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"244e0bd6-1fc6-471e-d40e-dee0694b8511"},"source":["vector = pretrained_vectors.get_vector('javascript', norm=True)\n","vector.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150,)"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:45:58.378657Z","iopub.status.busy":"2021-06-30T02:45:58.378555Z","iopub.status.idle":"2021-06-30T02:45:58.380890Z","shell.execute_reply":"2021-06-30T02:45:58.380500Z","shell.execute_reply.started":"2021-06-30T02:45:58.378643Z"},"id":"2TBk2EJ8JxnV","tags":[]},"source":["embedding_dim =150\n","test_weights = np.zeros((2, embedding_dim))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:45:58.382477Z","iopub.status.busy":"2021-06-30T02:45:58.382369Z","iopub.status.idle":"2021-06-30T02:45:58.384401Z","shell.execute_reply":"2021-06-30T02:45:58.384126Z","shell.execute_reply.started":"2021-06-30T02:45:58.382463Z"},"id":"dWubBv7LJxnV","tags":[]},"source":["test_weights[0] = pretrained_vectors.get_vector('javascript', norm=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:45:58.385063Z","iopub.status.busy":"2021-06-30T02:45:58.384960Z","iopub.status.idle":"2021-06-30T02:45:58.390940Z","shell.execute_reply":"2021-06-30T02:45:58.390590Z","shell.execute_reply.started":"2021-06-30T02:45:58.385050Z"},"id":"hVf1yJ8FJxnV","tags":[]},"source":["test_weights[1] =  np.random.normal(size=(embedding_dim, ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2021-06-30T02:45:58.391706Z","iopub.status.busy":"2021-06-30T02:45:58.391571Z","iopub.status.idle":"2021-06-30T02:45:58.397242Z","shell.execute_reply":"2021-06-30T02:45:58.396857Z","shell.execute_reply.started":"2021-06-30T02:45:58.391689Z"},"id":"HOz6iRnJJxnV","jupyter":{"outputs_hidden":true},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647091363,"user_tz":360,"elapsed":18,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"999a5170-1dc2-44d8-86d5-55e997b72842"},"source":["test_weights"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-8.49839970e-02, -2.17764992e-02, -6.82247356e-02,\n","         5.70525881e-03,  2.65885089e-02,  1.32282272e-01,\n","        -4.59169894e-02, -2.73072417e-03,  5.12668304e-02,\n","        -9.32804793e-02,  2.14768946e-02, -5.41905826e-03,\n","        -6.06425386e-03,  1.36130795e-01,  1.90235917e-02,\n","         1.46427318e-01,  5.81797771e-02, -6.00542547e-03,\n","         7.31237000e-03, -6.35026023e-02,  5.12750894e-02,\n","        -7.50809442e-04,  1.86569560e-02, -7.20354319e-02,\n","        -7.38556683e-02,  1.22542307e-01,  3.26105095e-02,\n","         3.38960290e-02,  1.43136814e-01,  1.45177590e-02,\n","         9.68288779e-02,  5.26264310e-02, -5.18140336e-03,\n","        -2.11011129e-03, -1.56064387e-02, -7.48816431e-02,\n","        -1.10546961e-01,  9.78591442e-02, -5.69242612e-02,\n","         1.93735644e-01,  2.65050828e-02, -9.60124657e-03,\n","         7.94634297e-02,  3.92516293e-02, -2.60088556e-02,\n","         1.00974530e-01, -8.80348682e-02,  1.30057856e-01,\n","        -4.09416333e-02, -1.32559299e-01,  4.95028086e-02,\n","        -8.84458274e-02, -7.06662014e-02, -8.37802514e-02,\n","        -1.41215289e-03,  5.36811277e-02, -1.64617613e-01,\n","        -3.76956277e-02, -1.02782816e-01, -7.55085349e-02,\n","        -2.52975449e-02,  7.38008469e-02, -1.81783957e-03,\n","        -1.27833607e-02, -7.27011263e-02,  6.24733679e-02,\n","        -7.60692433e-02, -1.17694244e-01,  3.04018427e-03,\n","        -3.37468609e-02, -2.05444992e-02, -8.12462419e-02,\n","        -1.18542142e-01,  9.22960714e-02,  1.89600009e-02,\n","        -7.68795563e-03,  2.88685970e-02,  3.78313810e-02,\n","        -2.35535845e-01, -6.09148182e-02,  1.15142480e-01,\n","        -7.29917660e-02,  2.34095822e-03,  4.55050021e-02,\n","        -1.47048514e-02,  3.35949622e-02, -7.31281340e-02,\n","         1.23739406e-01, -1.18727997e-01, -1.00180969e-01,\n","        -1.87381078e-03,  7.88293481e-02, -9.87407565e-03,\n","         8.38500336e-02,  5.75252250e-03,  1.32376984e-01,\n","        -2.27108300e-02, -8.17893445e-02,  1.19536512e-01,\n","         5.30564114e-02,  1.93254258e-02, -5.53140789e-02,\n","         3.05761443e-03,  9.38868746e-02, -1.22569434e-01,\n","         1.00283235e-01,  1.55801941e-02, -8.58978704e-02,\n","        -1.82096139e-01, -7.89970160e-02,  4.90375161e-02,\n","        -4.64441031e-02, -6.13145865e-02,  2.44968653e-01,\n","        -9.23011005e-02,  1.38278723e-01, -6.54256344e-02,\n","        -8.64765421e-02, -1.07043751e-01, -9.42380503e-02,\n","        -3.70656364e-02,  3.07282675e-02,  5.07859178e-02,\n","        -5.55103496e-02,  6.11605160e-02,  2.40837485e-02,\n","         5.52361878e-03, -8.83249119e-02, -1.28407553e-01,\n","         4.01185304e-02,  8.23620856e-02,  1.00126024e-02,\n","        -1.11854933e-02,  2.30715889e-02,  9.15833265e-02,\n","        -2.55778548e-03, -7.26779476e-02,  1.03531234e-01,\n","         6.26780987e-02, -1.18119463e-01, -1.23329058e-01,\n","        -4.83949145e-04,  1.18091151e-01,  1.54569745e-01,\n","        -5.36536202e-02, -1.63430527e-01, -2.18902938e-02,\n","        -6.43911436e-02, -1.89018454e-02,  6.52916953e-02],\n","       [ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,\n","         2.24089320e+00,  1.86755799e+00, -9.77277880e-01,\n","         9.50088418e-01, -1.51357208e-01, -1.03218852e-01,\n","         4.10598502e-01,  1.44043571e-01,  1.45427351e+00,\n","         7.61037725e-01,  1.21675016e-01,  4.43863233e-01,\n","         3.33674327e-01,  1.49407907e+00, -2.05158264e-01,\n","         3.13067702e-01, -8.54095739e-01, -2.55298982e+00,\n","         6.53618595e-01,  8.64436199e-01, -7.42165020e-01,\n","         2.26975462e+00, -1.45436567e+00,  4.57585173e-02,\n","        -1.87183850e-01,  1.53277921e+00,  1.46935877e+00,\n","         1.54947426e-01,  3.78162520e-01, -8.87785748e-01,\n","        -1.98079647e+00, -3.47912149e-01,  1.56348969e-01,\n","         1.23029068e+00,  1.20237985e+00, -3.87326817e-01,\n","        -3.02302751e-01, -1.04855297e+00, -1.42001794e+00,\n","        -1.70627019e+00,  1.95077540e+00, -5.09652182e-01,\n","        -4.38074302e-01, -1.25279536e+00,  7.77490356e-01,\n","        -1.61389785e+00, -2.12740280e-01, -8.95466561e-01,\n","         3.86902498e-01, -5.10805138e-01, -1.18063218e+00,\n","        -2.81822283e-02,  4.28331871e-01,  6.65172224e-02,\n","         3.02471898e-01, -6.34322094e-01, -3.62741166e-01,\n","        -6.72460448e-01, -3.59553162e-01, -8.13146282e-01,\n","        -1.72628260e+00,  1.77426142e-01, -4.01780936e-01,\n","        -1.63019835e+00,  4.62782256e-01, -9.07298364e-01,\n","         5.19453958e-02,  7.29090562e-01,  1.28982911e-01,\n","         1.13940068e+00, -1.23482582e+00,  4.02341641e-01,\n","        -6.84810091e-01, -8.70797149e-01, -5.78849665e-01,\n","        -3.11552532e-01,  5.61653422e-02, -1.16514984e+00,\n","         9.00826487e-01,  4.65662440e-01, -1.53624369e+00,\n","         1.48825219e+00,  1.89588918e+00,  1.17877957e+00,\n","        -1.79924836e-01, -1.07075262e+00,  1.05445173e+00,\n","        -4.03176947e-01,  1.22244507e+00,  2.08274978e-01,\n","         9.76639036e-01,  3.56366397e-01,  7.06573168e-01,\n","         1.05000207e-02,  1.78587049e+00,  1.26912093e-01,\n","         4.01989363e-01,  1.88315070e+00, -1.34775906e+00,\n","        -1.27048500e+00,  9.69396708e-01, -1.17312341e+00,\n","         1.94362119e+00, -4.13618981e-01, -7.47454811e-01,\n","         1.92294203e+00,  1.48051479e+00,  1.86755896e+00,\n","         9.06044658e-01, -8.61225685e-01,  1.91006495e+00,\n","        -2.68003371e-01,  8.02456396e-01,  9.47251968e-01,\n","        -1.55010093e-01,  6.14079370e-01,  9.22206672e-01,\n","         3.76425531e-01, -1.09940079e+00,  2.98238174e-01,\n","         1.32638590e+00, -6.94567860e-01, -1.49634540e-01,\n","        -4.35153552e-01,  1.84926373e+00,  6.72294757e-01,\n","         4.07461836e-01, -7.69916074e-01,  5.39249191e-01,\n","        -6.74332661e-01,  3.18305583e-02, -6.35846078e-01,\n","         6.76433295e-01,  5.76590817e-01, -2.08298756e-01,\n","         3.96006713e-01, -1.09306151e+00, -1.49125759e+00,\n","         4.39391701e-01,  1.66673495e-01,  6.35031437e-01,\n","         2.38314477e+00,  9.44479487e-01, -9.12822225e-01,\n","         1.11701629e+00, -1.31590741e+00, -4.61584605e-01]])"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:45:58.397963Z","iopub.status.busy":"2021-06-30T02:45:58.397843Z","iopub.status.idle":"2021-06-30T02:45:58.400627Z","shell.execute_reply":"2021-06-30T02:45:58.400360Z","shell.execute_reply.started":"2021-06-30T02:45:58.397947Z"},"id":"4dcaTAuwJxnW","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647091363,"user_tz":360,"elapsed":16,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"ffc25923-bbc5-4b0f-fb11-1b649919eab6"},"source":["len(vocab_dict)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25040"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:45:58.401175Z","iopub.status.busy":"2021-06-30T02:45:58.401077Z","iopub.status.idle":"2021-06-30T02:46:00.061074Z","shell.execute_reply":"2021-06-30T02:46:00.060592Z","shell.execute_reply.started":"2021-06-30T02:45:58.401162Z"},"id":"3TzSA-ysJxnW","tags":[]},"source":["embedding_dim = 150\n","pretrained_weights = np.zeros((len(vocab_dict), embedding_dim))\n","words_found = 0\n","words_not_found = 0\n","\n","for i, word in enumerate(vocab_dict.get_itos()):\n","    try: \n","        pretrained_weights[i] = pretrained_vectors.get_vector(word, norm=True)\n","        words_found += 1\n","    except KeyError:\n","        words_not_found  += 1\n","        pretrained_weights[i] = np.random.normal(size=(embedding_dim, ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:46:00.061675Z","iopub.status.busy":"2021-06-30T02:46:00.061553Z","iopub.status.idle":"2021-06-30T02:46:00.064335Z","shell.execute_reply":"2021-06-30T02:46:00.063980Z","shell.execute_reply.started":"2021-06-30T02:46:00.061661Z"},"id":"hh1c365fJxnW","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647093602,"user_tz":360,"elapsed":26,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"a35d70f7-3b60-4f83-d86e-f1c811617090"},"source":["words_found"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25040"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-06-30T02:46:00.064986Z","iopub.status.busy":"2021-06-30T02:46:00.064886Z","iopub.status.idle":"2021-06-30T02:46:00.069909Z","shell.execute_reply":"2021-06-30T02:46:00.069607Z","shell.execute_reply.started":"2021-06-30T02:46:00.064973Z"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"VzpZ2kIE8xwt","executionInfo":{"status":"ok","timestamp":1638647093603,"user_tz":360,"elapsed":24,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"d95aac43-013e-44dc-8930-5989212efd83"},"source":["words_not_found"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"5tL7j8siW9qm"},"source":["# Model "]},{"cell_type":"code","metadata":{"id":"rawoZIzu9CeC"},"source":["# Define custom model using nn.Module()\n","class MLPCustom2(nn.Module):\n","  def __init__(self, vocab_size, h_sizes_list, dprobs_list, batchnorm_binary, output_dim, non_linearity, pretrained_weights, task):\n","        \n","    super().__init__()\n","\n","    self.h_sizes_list = h_sizes_list # h_sizes = [emb_dim, hidden_dim1,....hidden_dim2,....hidden_dimn] # n + 1 elements\n","    \n","    self.dprobs_list = dprobs_list # dpropb =[prob1, prob2....probn] # n elements\n","    self.batchnorm_binary = batchnorm_binary # [True, False....] # n elements\n","\n","    \n","    self.non_linearity = non_linearity\n","    self.output_dim = output_dim\n","    self.vocab_size = vocab_size\n","    self.pretrained_weights = pretrained_weights\n","    self.task = task\n","\n","    # Initialize hidden layers  \n","\n","    self.hidden = nn.ModuleList()\n","    self.dropout = nn.ModuleList()\n","    self.batchnorm = nn.ModuleList()\n","\n","    if self.task == 2:\n","       self.embedding = nn.EmbeddingBag(vocab_size, self.h_sizes_list[0])\n","\n","    # Task 5\n","    if self.task == 5:\n","       self.embedding = nn.EmbeddingBag(vocab_size, self.h_sizes_list[0]).from_pretrained(pretrained_weights,\n","                                                                               freeze = True)\n","       \n","    # Task 6\n","    if self.task == 6:\n","       self.embedding = nn.EmbeddingBag(vocab_size, self.h_sizes_list[0]).from_pretrained(pretrained_weights,\n","                                                                               freeze = False)\n","\n","    for k in range(len(h_sizes_list)-1):\n","      self.hidden.append(nn.Linear(self.h_sizes_list[k], h_sizes_list[k+1]))\n","      self.dropout.append(nn.Dropout(p=dprobs_list[k]))\n","\n","      if self.batchnorm_binary:\n","        self.batchnorm.append(nn.BatchNorm1d(self.h_sizes_list[k+1], momentum=0.9))\n","      \n","    \n","    self.output_layer = nn.Linear(self.h_sizes_list[-1], output_dim)\n","   \n","    \n","    ## it is better to use nn.functional.relu in the forward function\n","    # self.relu = nn.ReLU()\n","\n","  def forward(self, input, offsets):\n","    x = self.embedding(input, offsets)\n","    for  k in range(len(self.h_sizes_list)-1):\n","      x =  self.non_linearity(self.hidden[k](x))\n","      if self.batchnorm_binary:\n","        x = self.batchnorm[k](x)\n","      x= self.dropout[k](x)\n","\n","    x = self.output_layer(x)\n","    # we are not using softmax function in the forward passs\n","    # nn.crossentropy loss (which we will use to define our loss) combines  nn.LogSoftmax() and nn.NLLLoss() in one single class\n","    return x  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ry-7lHYVPlPP"},"source":["# Training Functions"]},{"cell_type":"markdown","metadata":{"id":"z3GdVoTjh7YD"},"source":["## Training Epoch "]},{"cell_type":"code","metadata":{"id":"Pv4x22lZMn5p"},"source":["def train(train_loader, model, optimizer, loss_function, log_batch, log_interval, grad_clipping, max_norm):\n","\n","  \"\"\" \n","  Function for training the model in each epoch\n","  Input: iterator for train dataset, initial weights and bias, epochs, learning rate.\n","  Output: final weights, bias, train loss, train accuracy\n","  \"\"\"\n","  # initilalize variables as global\n","  # these counts will be updated every epoch\n","  global example_ct_train\n","  global batch_ct_train\n","\n","  # Training Loop loop\n","  # Initialize train_loss at the he start of the epoch\n","  running_train_loss = 0\n","  running_train_correct = 0\n","  \n","  # put the model in training mode\n","  model.train()\n","\n","  # Iterate on batches from the dataset using train_loader\n","  for input, targets, offsets in train_loader:\n","    \n","    # move inputs and outputs to GPUs\n","    input = input.to(device)\n","    targets = targets.to(device).float()\n","    offsets = offsets.to(device)\n","\n","    # Forward pass\n","    output = model(input,offsets)\n","    loss = loss_function(output, targets)\n","\n","    # Correct prediction\n","    # Correct prediction\n","    #Sigmoid = nn.Sigmoid()\n","    #output = Sigmoid(output) \n","    #threshold = torch.tensor([0.5]).to(device)\n","    #print('output of sigmoitd',output)\n","    y_pred = (output>0).float()\n","    #print('predicted value',y_pred)\n","    #y_pred = torch.argmax(output, dim = 1)\n","    #correct = torch.sum(y_pred == targets)          #correct = sum_pred(y_pred,targets)                                      #\n","    #print('Correct value',correct)\n","\n","    example_ct_train +=  len(targets)\n","    batch_ct_train += 1\n","\n","    # set gradients to zero \n","    optimizer.zero_grad()\n","\n","    # Backward pass\n","    loss.backward()\n","\n","    # Gradient Clipping\n","    if grad_clipping:\n","      nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm, norm_type=2)\n","\n","    # Update parameters using their gradient\n","    optimizer.step()\n","          \n","    # Add train loss of a batch \n","    running_train_loss += loss.item()\n","\n","    # Add Corect counts of a batch\n","    #running_train_correct += correct\n","\n","    # log batch loss and accuracy\n","    if log_batch:\n","      if ((batch_ct_train + 1) % log_interval) == 0:\n","        wandb.log({f\"Train Batch Loss  :\": loss})\n","        #wandb.log({f\"Train Batch Acc :\": correct/(len(targets)*10)})\n","\n","  \n","  # Calculate mean train loss for the whole dataset for a particular epoch\n","  train_loss = running_train_loss/len(train_loader)\n","\n","\n","\n","  # Calculate accuracy for the whole dataset for a particular epoch\n","  #train_acc = running_train_correct/(len(train_loader.dataset)*10)\n","\n","  return train_loss #train_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aic8wk5niFCz"},"source":["## Validation/Test Epoch"]},{"cell_type":"code","metadata":{"id":"pHP1WKDessiI"},"source":["def valid(loader, model, optimizer, loss_function, log_batch, log_interval):\n","\n","  \"\"\" \n","  Function for training the model and plotting the graph for train & valid loss vs epoch.\n","  Input: iterator for train dataset, initial weights and bias, epochs, learning rate, batch size.\n","  Output: final weights, bias and train loss and valid loss for each epoch.\n","  \"\"\"\n","\n","  # initilalize variables as global\n","  # these counts will be updated every epoch\n","  global example_ct_valid\n","  global batch_ct_valid\n","\n","  # Validation loop\n","  # Initialize train_loss at the he strat of the epoch\n","  running_valid_loss = 0\n","  running_valid_correct = 0\n","  \n","  # put the model in evaluation mode\n","  model.eval()\n","\n","  with torch.no_grad():\n","    for input,targets, offsets in loader:\n","\n","      # move inputs and outputs to GPUs\n","      input = input.to(device)\n","      targets = targets.to(device).float()\n","      offsets = offsets.to(device)\n","\n","      # Forward pass\n","      output = model(input,offsets)\n","      \n","      loss = loss_function(output,targets)\n","\n","      # Correct Predictions\n","      # Correct prediction\n","      #Sigmoid = nn.Sigmoid()\n","      #output = Sigmoid(output) \n","      #threshold = torch.tensor([0.5]).to(device)\n","      #print('output of sigmoitd',output)\n","      y_pred = (output>0).float()\n","      #print('predicted value',y_pred)\n","      #y_pred = torch.argmax(output, dim = 1)\n","      #correct = torch.sum(y_pred == targets)          #correct = sum_pred(y_pred,targets)                                      #\n","      #print('Correct value',correct)\n","\n","      # count of images and batches\n","      example_ct_valid +=  len(targets)\n","      batch_ct_valid += 1\n","\n","      # Add valid loss of a batch \n","      running_valid_loss += loss.item()\n","\n","      # Add correct count for each batch\n","      #running_valid_correct += correct\n","\n","      # log batch loss and accuracy\n","      if log_batch:\n","        if ((batch_ct_valid + 1) % log_interval) == 0:\n","          wandb.log({f\"Valid Batch Loss  :\": loss})\n","          #wandb.log({f\"Valid Batch Accuracy :\": correct/(len(targets)*10)})\n","\n","\n","    # Calculate mean valid loss for the whole dataset for a particular epoch\n","    valid_loss = running_valid_loss/len(valid_loader)\n","\n","    # scheduler step\n","    # scheduler.step(valid_loss)\n","    # scheduler.step()\n","\n","    # Calculate accuracy for the whole dataset for a particular epoch\n","    #valid_acc = running_valid_correct/(len(valid_loader.dataset)*10)\n","    \n","  return valid_loss #valid_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwF70eqE6n_v"},"source":["##  Model Training Loop\n"]},{"cell_type":"code","metadata":{"id":"KeCKVgg-5FiZ"},"source":["def train_loop(train_loader, valid_loader, model, loss_function, optimizer, epochs, device, patience, early_stopping,\n","               file_model):\n","\n","  '''\n","  model: specify your model for training\n","  criterion: loss function \n","  optimizer: optimizer like SGD , ADAM etc.\n","  train loader: function to carete batches for training data\n","  loader : function to create batches for valid data set\n","  file_model : specify file name for saving your model. This way we can upload the model weights from file. We will not to run model again.\n","  \n","\n","  '''\n","  # Create lists to store train and valid loss at each epoch\n","\n","  train_loss_history = []\n","  valid_loss_history = []\n","  #train_acc_history = []\n","  #valid_acc_history = []\n","  delta = 0\n","  best_score = None\n","  valid_loss_min = np.Inf\n","  counter_early_stop=0\n","  early_stop=False\n","\n","\n","  # Iterate for the given number of epochs\n","  for epoch in range(epochs):\n","    t0 = datetime.now()\n","    # Get train loss and accuracy for one epoch\n","\n","    train_loss = train(train_loader, model, optimizer, loss_function, \n","                                  wandb.config.log_batch, wandb.config.log_interval,\n","                                  wandb.config.grad_clipping, wandb.config.max_norm)\n","    valid_loss = valid(valid_loader, model, optimizer, loss_function,\n","                                    wandb.config.log_batch, wandb.config.log_interval)\n","\n","    dt = datetime.now() - t0\n","\n","    # Save history of the Losses and accuracy\n","    train_loss_history.append(train_loss)\n","    #train_acc_history.append(train_acc)\n","    valid_loss_history.append(valid_loss)\n","    #valid_acc_history.append(valid_acc)\n","\n","    if early_stopping:\n","      score = -valid_loss\n","      if best_score is None:\n","        best_score=score\n","        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n","        torch.save(model.state_dict(), file_model)\n","        valid_loss_min = valid_loss\n","\n","      elif score < best_score + delta:\n","        counter_early_stop += 1\n","        print(f'Early stoping counter: {counter_early_stop} out of {patience}')\n","        if counter_early_stop > patience:\n","          early_stop = True\n","\n","      \n","      else:\n","        best_score = score\n","        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n","        torch.save(model.state_dict(), file_model)\n","        counter_early_stop=0\n","        valid_loss_min = valid_loss\n","\n","      if early_stop:\n","        print('Early Stopping')\n","        break\n","\n","    else:\n","\n","      score = -valid_loss\n","      if best_score is None:\n","        best_score=score\n","        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n","        torch.save(model.state_dict(), file_model)\n","        valid_loss_min = valid_loss\n","\n","      elif score < best_score + delta:\n","        print(f'Validation loss has not decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Not Saving Model...')\n","      \n","      else:\n","        best_score = score\n","        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n","        torch.save(model.state_dict(), file_model)\n","        valid_loss_min = valid_loss\n","\n","\n","\n","    # Log the train and valid loss to W&B\n","    wandb.log({f\"Train epoch Loss :\": train_loss, f\"Valid epoch Loss :\": valid_loss })\n","    #wandb.log({f\"Train epoch Acc :\": train_acc, f\"Valid epoch Acc :\": valid_acc})\n","\n","\n","\n","    # Print the train loss and accuracy for given number of epochs, batch size and number of samples\n","    print(f'Epoch : {epoch+1} / {epochs}')\n","    print(f'Time to complete {epoch+1} is {dt}')\n","    # print(f'Learning rate: {scheduler._last_lr[0]}')\n","    print(f'Train Loss: {train_loss : .4f} | Train Accuracy: ')\n","    print(f'Valid Loss: {valid_loss : .4f} | Valid Accuracy: ')\n","    print()\n","    torch.cuda.empty_cache()\n","\n","  return train_loss_history, valid_loss_history #, valid_acc_history,train_acc_history,\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yte1HzNniWlr"},"source":["#Model Training"]},{"cell_type":"code","metadata":{"id":"P6w2qWPR9zeO"},"source":["hyperparameters = dict(\n","    \n","    h_sizes_list = [150] + [200], # 300 = embed_dim\n","    dprobs_list = [0],\n","    batchnorm_binary = False,\n","    task = 6,\n","    vocab_size = len(vocab_dict),\n","    output_dim = 10,\n","    epochs = 20,\n","    batch_size = 128,\n","    learning_rate = 0.01,\n","    dataset=\"Multilabel\",\n","    architecture=\"MLP\",\n","    log_interval = 25,\n","    log_batch = True,\n","    file_model = save_model_folder/'regular_exp1.pt',\n","    grad_clipping = True,\n","    max_norm = 1,\n","    momentum = 0,\n","    patience = 10,\n","    early_stopping = True,\n","    scheduler_factor = 0.5,\n","    scheduler_patience = 0,\n","    weight_decay = 0.002\n","   )\n","\n","# non_linearity = F.elu \n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","non_linearity = F.selu\n","pretrained_weights_tensor = torch.tensor(pretrained_weights).float()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAIIhz2uP6AV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647093603,"user_tz":360,"elapsed":22,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"59585c9a-28e2-4a68-a2a0-96528c4a5d7a"},"source":["pretrained_weights_tensor.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([25040, 150])"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"9Zp9fDrAheXc","executionInfo":{"status":"ok","timestamp":1638647098012,"user_tz":360,"elapsed":4429,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"7277738b-a362-4954-ff88-a7e875a3da3e"},"source":["wandb.init(name = 'shishir_hw4_Q5_exp2', project = 'shishir_hw4_nlp', config = hyperparameters)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msxs200126\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/sxs200126/shishir_hw4_nlp/runs/29ez4g2a\" target=\"_blank\">shishir_hw4_Q5_exp2</a></strong> to <a href=\"https://wandb.ai/sxs200126/shishir_hw4_nlp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fcb3ab6acd0>"],"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sxs200126/shishir_hw4_nlp/runs/29ez4g2a?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GUCW6ipgS8p","executionInfo":{"status":"ok","timestamp":1638647098013,"user_tz":360,"elapsed":8,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"fdc62f84-3fc9-43aa-a32b-febab456c031"},"source":["# wandb.config.non_linearity = non_linearity\n","wandb.config.device = device\n","print(wandb.config.device )\n","wandb.config.non_linearity = non_linearity\n","print(wandb.config.non_linearity)\n","# print(wandb.config.non_linearity)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n","torch.nn.functional.selu\n"]}]},{"cell_type":"markdown","metadata":{"id":"B9DjhWa3k-Gk"},"source":["## Specify Dataloader, Loss_function, Model, Optimizer, Weight Initialization"]},{"cell_type":"code","metadata":{"id":"XyTAuL2BpO_Q"},"source":["# Fix seed value\n","SEED = 2345\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","# Data Loader\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=wandb.config.batch_size, \n","                                           shuffle = True, collate_fn=collate_batch, num_workers=2)\n","valid_loader = torch.utils.data.DataLoader(validset, batch_size=wandb.config.batch_size, \n","                                           shuffle = False, collate_fn=collate_batch, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=wandb.config.batch_size,   \n","                                         shuffle = False, collate_fn=collate_batch, num_workers=2)\n","\n","# cross entropy loss function\n","loss_function = nn.BCEWithLogitsLoss()\n","\n","# model \n","model = MLPCustom2(wandb.config.vocab_size, wandb.config.h_sizes_list, wandb.config.dprobs_list, wandb.config.batchnorm_binary, \n","           wandb.config.output_dim, non_linearity, pretrained_weights_tensor, wandb.config.task)\n","\n","def init_weights(m):\n","  if type(m) == nn.Linear:\n","        torch.nn.init.kaiming_normal_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n","\n","  if type(m) == nn.EmbeddingBag:\n","    torch.nn.init.normal_(m.weight, mean = 0, std = 1)\n","\n","  \n","\n","# apply init function recursibvely to all the modules\n","#model.apply(init_weights)\n","\n","# put model to GPUs\n","model.to(device)\n","\n","#Intialize stochiastic gradient descent optimizer\n","#optimizer = torch.optim.SGD(model.parameters(), lr = wandb.config.learning_rate, weight_decay=wandb.config.weight_decay, \n","#                          momentum=wandb.config.momentum)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n","\n","wandb.config.optimizer = optimizer\n","\n","\n","\n","#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor= wandb.config.scheduler_factor, \n","#                              patience=wandb.config.scheduler_patience, verbose=True)\n","\n","# scheduler = StepLR(optimizer, gamma=0.4,step_size=1, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"htWT1-Nhlw3m"},"source":["## Train Model and Save best model"]},{"cell_type":"code","metadata":{"id":"MwnGtxo3B2nD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647109807,"user_tz":360,"elapsed":30,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"461088d0-7dc8-409a-bed6-d6b9b6e42133"},"source":["wandb.watch(model, log = 'all', log_freq=25, log_graph=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"]},{"output_type":"execute_result","data":{"text/plain":["[<wandb.wandb_torch.TorchGraph at 0x7fcb3a9a5150>]"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"QyAaOEg5DdKp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638647159765,"user_tz":360,"elapsed":49979,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"ccdba0b3-c0a9-4e10-f8bb-d5b2d43be5db"},"source":["example_ct_train, batch_ct_train, example_ct_valid, batch_ct_valid = 0, 0, 0, 0\n","train_loss_history, valid_loss_history  = train_loop(train_loader, valid_loader, model, loss_function, optimizer, \n","                                                                                          wandb.config.epochs, wandb.config.device,\n","                                                                                          wandb.config.patience, wandb.config.early_stopping,\n","                                                                                          wandb.config.file_model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss has decreased (inf --> 0.382592). Saving Model...\n","Epoch : 1 / 20\n","Time to complete 1 is 0:00:02.573183\n","Train Loss:  0.4151 | Train Accuracy: \n","Valid Loss:  0.3826 | Valid Accuracy: \n","\n","Validation loss has decreased (0.382592 --> 0.313292). Saving model...\n","Epoch : 2 / 20\n","Time to complete 2 is 0:00:02.490774\n","Train Loss:  0.3341 | Train Accuracy: \n","Valid Loss:  0.3133 | Valid Accuracy: \n","\n","Validation loss has decreased (0.313292 --> 0.306464). Saving model...\n","Epoch : 3 / 20\n","Time to complete 3 is 0:00:02.436871\n","Train Loss:  0.3101 | Train Accuracy: \n","Valid Loss:  0.3065 | Valid Accuracy: \n","\n","Early stoping counter: 1 out of 10\n","Epoch : 4 / 20\n","Time to complete 4 is 0:00:02.463132\n","Train Loss:  0.3091 | Train Accuracy: \n","Valid Loss:  0.3098 | Valid Accuracy: \n","\n","Validation loss has decreased (0.306464 --> 0.302874). Saving model...\n","Epoch : 5 / 20\n","Time to complete 5 is 0:00:02.394326\n","Train Loss:  0.3094 | Train Accuracy: \n","Valid Loss:  0.3029 | Valid Accuracy: \n","\n","Early stoping counter: 1 out of 10\n","Epoch : 6 / 20\n","Time to complete 6 is 0:00:02.419695\n","Train Loss:  0.3102 | Train Accuracy: \n","Valid Loss:  0.3068 | Valid Accuracy: \n","\n","Early stoping counter: 2 out of 10\n","Epoch : 7 / 20\n","Time to complete 7 is 0:00:02.403558\n","Train Loss:  0.3081 | Train Accuracy: \n","Valid Loss:  0.3049 | Valid Accuracy: \n","\n","Early stoping counter: 3 out of 10\n","Epoch : 8 / 20\n","Time to complete 8 is 0:00:02.507787\n","Train Loss:  0.3076 | Train Accuracy: \n","Valid Loss:  0.3074 | Valid Accuracy: \n","\n","Early stoping counter: 4 out of 10\n","Epoch : 9 / 20\n","Time to complete 9 is 0:00:02.354102\n","Train Loss:  0.3078 | Train Accuracy: \n","Valid Loss:  0.3041 | Valid Accuracy: \n","\n","Early stoping counter: 5 out of 10\n","Epoch : 10 / 20\n","Time to complete 10 is 0:00:02.499344\n","Train Loss:  0.3066 | Train Accuracy: \n","Valid Loss:  0.3039 | Valid Accuracy: \n","\n","Early stoping counter: 6 out of 10\n","Epoch : 11 / 20\n","Time to complete 11 is 0:00:02.425274\n","Train Loss:  0.3073 | Train Accuracy: \n","Valid Loss:  0.3039 | Valid Accuracy: \n","\n","Early stoping counter: 7 out of 10\n","Epoch : 12 / 20\n","Time to complete 12 is 0:00:02.409884\n","Train Loss:  0.3063 | Train Accuracy: \n","Valid Loss:  0.3037 | Valid Accuracy: \n","\n","Validation loss has decreased (0.302874 --> 0.302476). Saving model...\n","Epoch : 13 / 20\n","Time to complete 13 is 0:00:02.418600\n","Train Loss:  0.3065 | Train Accuracy: \n","Valid Loss:  0.3025 | Valid Accuracy: \n","\n","Validation loss has decreased (0.302476 --> 0.301610). Saving model...\n","Epoch : 14 / 20\n","Time to complete 14 is 0:00:02.486222\n","Train Loss:  0.3066 | Train Accuracy: \n","Valid Loss:  0.3016 | Valid Accuracy: \n","\n","Early stoping counter: 1 out of 10\n","Epoch : 15 / 20\n","Time to complete 15 is 0:00:02.406161\n","Train Loss:  0.3077 | Train Accuracy: \n","Valid Loss:  0.3020 | Valid Accuracy: \n","\n","Early stoping counter: 2 out of 10\n","Epoch : 16 / 20\n","Time to complete 16 is 0:00:02.383583\n","Train Loss:  0.3062 | Train Accuracy: \n","Valid Loss:  0.3044 | Valid Accuracy: \n","\n","Validation loss has decreased (0.301610 --> 0.300997). Saving model...\n","Epoch : 17 / 20\n","Time to complete 17 is 0:00:02.311516\n","Train Loss:  0.3066 | Train Accuracy: \n","Valid Loss:  0.3010 | Valid Accuracy: \n","\n","Early stoping counter: 1 out of 10\n","Epoch : 18 / 20\n","Time to complete 18 is 0:00:02.446990\n","Train Loss:  0.3072 | Train Accuracy: \n","Valid Loss:  0.3041 | Valid Accuracy: \n","\n","Early stoping counter: 2 out of 10\n","Epoch : 19 / 20\n","Time to complete 19 is 0:00:02.424094\n","Train Loss:  0.3056 | Train Accuracy: \n","Valid Loss:  0.3020 | Valid Accuracy: \n","\n","Early stoping counter: 3 out of 10\n","Epoch : 20 / 20\n","Time to complete 20 is 0:00:02.419177\n","Train Loss:  0.3055 | Train Accuracy: \n","Valid Loss:  0.3013 | Valid Accuracy: \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"FWCLH47azD6j"},"source":["# **Accuracy and Predictions**\n","\n","Now we have final values for weights and bias after training the model. We will use these values to make predictions on the test dataset."]},{"cell_type":"markdown","metadata":{"id":"uzDX4D6EvcVC"},"source":["## Function to get predictions"]},{"cell_type":"code","metadata":{"id":"M6KZqsnqQFVu"},"source":["def get_acc_pred(data_loader, model):\n","  \"\"\" \n","  Function to get predictions for a given test set and calculate accuracy.\n","  Input: Iterator to the test set.\n","  Output: Prections and Accuracy for test set.\n","  \"\"\"\n","  with torch.no_grad():\n","    # Array to store predicted labels\n","    predictions = torch.Tensor()\n","    predictions = predictions.to(device)\n","\n","    # Array to store actual labels\n","    y = torch.Tensor()\n","    y = y.to(device)\n","    # Iterate over batches from test set\n","    for input, targets in data_loader:\n","      \n","      # move inputs and outputs to GPUs\n","      input = input.to(device)\n","      targets = targets.to(device)\n","\n","      # Calculated the predicted labels\n","      output = model(input)\n","\n","      # Choose the label with maximum probability\n","      indices = torch.argmax(output, dim = -2)\n","\n","      # Add the predicted labels to the array\n","      predictions = torch.cat((predictions, indices)) \n","\n","      # Add the actual labels to the array\n","      y = torch.cat((y, targets)) \n","\n","    # Check for complete dataset if actual and predicted labels are same or not\n","    # Calculate accuracy\n","    acc = (predictions == y).float().mean()\n","\n","  # Return array containing predictions and accuracy\n","  return predictions, acc\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lK-g-bSTvkl8"},"source":["## Load saved model from file "]}]}