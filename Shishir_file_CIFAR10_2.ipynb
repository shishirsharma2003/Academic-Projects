{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Shishir_file_hw5_Q2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPXQPrNGcnZNMHMvT+Y1SzZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"01751c0758e14356bd4cbe22a9a5c026":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_view_name":"VBoxView","_dom_classes":[],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9748374ee0d647e395bcb0ac5f3d5fdd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a26feabb3c444cf6b03472e37fcd83aa","IPY_MODEL_9eda2dd8c007486a84648254d17e1d4a"]}},"9748374ee0d647e395bcb0ac5f3d5fdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a26feabb3c444cf6b03472e37fcd83aa":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_view_name":"LabelView","style":"IPY_MODEL_14a37670d01142af873f59e3058bbdca","_dom_classes":[],"description":"","_model_name":"LabelModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0.10MB of 0.10MB uploaded (0.00MB deduped)\r","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1bebf1bc695f440490f86b4b15e63261"}},"9eda2dd8c007486a84648254d17e1d4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c5cfdaea6d9246748086d5af7b3f9303","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4045ebcbe4b6427d879b9453568260b6"}},"14a37670d01142af873f59e3058bbdca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1bebf1bc695f440490f86b4b15e63261":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c5cfdaea6d9246748086d5af7b3f9303":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4045ebcbe4b6427d879b9453568260b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"2OVn4YB-gzx-"},"source":["## **Wandb Link:** \n","\n","https://wandb.ai/sxs200126/CIFAR10_CNN/reports/Shishir_hw5--VmlldzoxMjI0NTEy?accessToken=quhmu5ctphel12f82j8k7d61zdz06xvonxdt8l1frfaahwn4g3hpfe9pmzomblfo"]},{"cell_type":"markdown","metadata":{"id":"sCVFh3-r73Fv"},"source":["## **Question 2 (15 Points):** \n","In this question, you can experiment with different convent architectures on CIFAR-10. You can experiment with different architectures, hyper-parameters, loss functions, and optimizers to train a model that achieves close to 80% accuracy on the CIFAR-10 validation set within 10 epochs\n"]},{"cell_type":"code","metadata":{"id":"DC5mNLEmGnon","executionInfo":{"status":"ok","timestamp":1637170063460,"user_tz":360,"elapsed":7142,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}}},"source":["%%capture\n","!pip install wandb --upgrade"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102},"id":"Nn9N9xOZGs1g","executionInfo":{"status":"ok","timestamp":1637170113466,"user_tz":360,"elapsed":47117,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"0894825b-c95a-4187-af96-ed8fe04742f5"},"source":["# Import wandb\n","import wandb\n","\n","# Login to W&B\n","wandb.login()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"]},{"name":"stdout","output_type":"stream","text":["wandb: Paste an API key from your profile and hit enter: ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4xJinn5Cvugc","executionInfo":{"status":"ok","timestamp":1637170122590,"user_tz":360,"elapsed":3115,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"6c3176cf-8b1b-412d-918e-3a042d08cf9d"},"source":["pip install torch-lr-finder"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-lr-finder\n","  Downloading torch_lr_finder-0.2.1-py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (21.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (3.2.2)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (1.10.0+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->torch-lr-finder) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->torch-lr-finder) (1.15.0)\n","Installing collected packages: torch-lr-finder\n","Successfully installed torch-lr-finder-0.2.1\n"]}]},{"cell_type":"code","metadata":{"id":"TqYqOtp5yluv","executionInfo":{"status":"ok","timestamp":1637170143789,"user_tz":360,"elapsed":15572,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}}},"source":["# Importing the necessary libraries\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","from torchsummary import summary\n","\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR, CyclicLR, OneCycleLR, StepLR\n","from torch_lr_finder import LRFinder\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import random\n","\n","from datetime import datetime\n","from pathlib import Path\n","import plotly.io as pio\n","pio.renderers.default = 'colab'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQQbjmDl5Got"},"source":["# Import random function\n","import random\n","\n","# Fix seed value\n","SEED = 2345\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJPqqFJulaGW","executionInfo":{"status":"ok","timestamp":1636953445231,"user_tz":360,"elapsed":37,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"12680658-9a85-4bc5-fbde-381ae6435d88"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"omgjHGvS2VBQ"},"source":["data_folder = Path('/content/drive/MyDrive/Data/DL')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27OJT5oq2WXI"},"source":["lecture_folder = Path('/content/drive/MyDrive/Deep Learning/HW5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcj3wFSn2WeS"},"source":["The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."]},{"cell_type":"markdown","metadata":{"id":"FAof5BqiyD1L"},"source":["## **Data Download and Transform to tensor**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvvRv9msIoWb","executionInfo":{"status":"ok","timestamp":1636953448340,"user_tz":360,"elapsed":3127,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"22f42124-f718-47ba-ef8a-ef3691f5c5e3"},"source":["# Transform to convert images to pytorch tensors and normalize the data\n","trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914,0.4822,0.4655), (0.3530,0.1994,0.2010))])\n","train_full = torchvision.datasets.CIFAR10(root=data_folder,\n","                                              train=True, \n","                                              transform=trans,\n","                                              download=True)\n","trainset, validset = torch.utils.data.random_split(train_full, [40000, 10000], generator=torch.Generator().manual_seed(42) )\n","testset  = torchvision.datasets.CIFAR10(root=data_folder,\n","                                              train=False, \n","                                              transform=trans,\n","                                              download=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJ3i3ItstVCC","executionInfo":{"status":"ok","timestamp":1636953448343,"user_tz":360,"elapsed":24,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"e6f00f7a-e716-4963-d29f-75b7c433b203"},"source":["classes = train_full.classes\n","classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['airplane',\n"," 'automobile',\n"," 'bird',\n"," 'cat',\n"," 'deer',\n"," 'dog',\n"," 'frog',\n"," 'horse',\n"," 'ship',\n"," 'truck']"]},"metadata":{},"execution_count":1102}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yi8d9uFtxgbZ","executionInfo":{"status":"ok","timestamp":1636953460207,"user_tz":360,"elapsed":11875,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"12136015-a0ba-45b5-ed20-3a18fdb05759"},"source":["class_count = {}\n","for _, index in train_full:\n","    label = classes[index]\n","    if label not in class_count:\n","        class_count[label] = 0\n","    class_count[label] += 1\n","class_count"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'airplane': 5000,\n"," 'automobile': 5000,\n"," 'bird': 5000,\n"," 'cat': 5000,\n"," 'deer': 5000,\n"," 'dog': 5000,\n"," 'frog': 5000,\n"," 'horse': 5000,\n"," 'ship': 5000,\n"," 'truck': 5000}"]},"metadata":{},"execution_count":1103}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23I5nK2C3zPj","executionInfo":{"status":"ok","timestamp":1636953460209,"user_tz":360,"elapsed":77,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"005293db-3297-4f46-8ebe-89c165fd7147"},"source":["train_full.data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 32, 32, 3)"]},"metadata":{},"execution_count":1104}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4H5TgmFk4mS4","executionInfo":{"status":"ok","timestamp":1636953460210,"user_tz":360,"elapsed":68,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"9be32091-9380-45b2-efaa-b6c4caf3b33d"},"source":["testset.data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 32, 32, 3)"]},"metadata":{},"execution_count":1105}]},{"cell_type":"code","metadata":{"id":"2J4yruMLxI8T"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U5PslDp5R7Ff"},"source":["### Check Transformation\n","\n","- Check if transformation are working correctly.\n","- The transformations are applied at the time of calling dataloader"]},{"cell_type":"code","metadata":{"id":"xj4RiSmGM984"},"source":["check_loader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrO2i0utr2fS","executionInfo":{"status":"ok","timestamp":1636953460217,"user_tz":360,"elapsed":46,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"ffcc4243-ed5a-489b-9af1-5921a4fe093c"},"source":["# check number of batches\n","len(check_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1250"]},"metadata":{},"execution_count":1107}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KSYReXcBsEjs","executionInfo":{"status":"ok","timestamp":1636953460219,"user_tz":360,"elapsed":42,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"a9ab5165-f103-4d41-a68c-8c1752addda4"},"source":["# check total training examples\n","len(check_loader.dataset)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["40000"]},"metadata":{},"execution_count":1108}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JGIeEEzEjRlP","executionInfo":{"status":"ok","timestamp":1636953460221,"user_tz":360,"elapsed":38,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"c509d014-157b-4eef-c9cd-cd4ae1040bb5"},"source":["# check imputs and outputs \n","for input, target in check_loader:\n","  print(f'shape of inputs is :{input.shape}')\n","  print(f'\\nmax input value  :{input.max()}')\n","  print(f'\\nmin input value  :{input.min()}')\n","  print(f'\\nmean input value  :{input.mean()}')\n","  print(f'\\nstd input value  :{input.std()}')\n","  print(f'\\nshape of targets is :{target.shape}')\n","   \n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["shape of inputs is :torch.Size([32, 3, 32, 32])\n","\n","max input value  :2.6592040061950684\n","\n","min input value  :-2.418254852294922\n","\n","mean input value  :-0.08662816882133484\n","\n","std input value  :1.1399555206298828\n","\n","shape of targets is :torch.Size([32])\n"]}]},{"cell_type":"markdown","metadata":{"id":"QVUnc93NFv6V"},"source":["First let's define labels for our dataset as dataset contains numerical values for now."]},{"cell_type":"markdown","metadata":{"id":"mS6k4qqgb3WC"},"source":["## Get Labels"]},{"cell_type":"code","metadata":{"id":"Qsuren0LCxEo"},"source":["def get_CIFAR10_labels(labels):  \n","    \"\"\" \n","    Function to generate labels.\n","    Input: numerical labels\n","    Output: actual string labels\n","    \"\"\"\n","\n","    # Create a list of labels\n","    text_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","                   'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","    # Return text_labels according to numerical values\n","    return [text_labels[int(i)] for i in labels]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"14DRHrSzhvQX"},"source":["## **Small Dataset**"]},{"cell_type":"code","metadata":{"id":"iEXDVS1PSEN8"},"source":["# n sample points\n","train_sample_size = int(len(trainset)/10)\n","valid_sample_size = int(len(validset)/10)\n","\n","# Getting n random indices\n","train_subset_indices = random.sample(range(0, len(trainset)), train_sample_size)\n","valid_subset_indices = random.sample(range(0, len(testset)), valid_sample_size)\n","\n","# Getting subset of dataset\n","train_subset = torch.utils.data.Subset(trainset, train_subset_indices)\n","valid_subset = torch.utils.data.Subset(validset, valid_subset_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5tL7j8siW9qm"},"source":["# Model CNN"]},{"cell_type":"code","metadata":{"id":"FvwkXedfDkDa"},"source":["class CIFAR10CNN(nn.Module):\n","    \n","    def __init__(self):\n","\n","      super().__init__()\n","\n","      super(CIFAR10CNN, self).__init__()\n","      \n","\n","      \n","      self.conv1_layer = nn.Sequential(\n","          nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding='same'), # 32 * 32\n","          nn.ReLU(),\n","          nn.BatchNorm2d(32),\n","          nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding='same'), # 32 * 32\n","          nn.ReLU(),\n","          nn.MaxPool2d(kernel_size=2, stride = 2 ) # 16 * 16\n","      )\n","\n","      self.conv2_layer = nn.Sequential(\n","          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'), # 16 * 16\n","          nn.ReLU(),\n","          nn.BatchNorm2d(64),\n","          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same'), # 16 * 16\n","          nn.ReLU(),\n","          nn.MaxPool2d(kernel_size=2) # 8 * 8\n","      )\n","\n","      self.conv3_layer = nn.Sequential(\n","          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'), # 8 * 8\n","          nn.ReLU(),\n","          nn.BatchNorm2d(128),\n","          nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same'), ## 8 * 8\n","          nn.ReLU(),\n","          nn.MaxPool2d(kernel_size=2) # 4 * 4\n","      )\n","      \n","\n","     \n","      self.flatten = nn.Flatten()\n","      \n","      self.fc1 = nn.Linear(4*4*128, out_features=128)\n","      \n","      self.fc2 = nn.Linear(128, 10)\n","\n","      #self.fc3 = nn.Linear(512, 10)\n","\n","      self.drop1 = nn.Dropout(0.2)\n","      self.drop2 = nn.Dropout(0.2)\n","      \n","      \n","      \n","        \n","    def forward(self, x):\n","        # conv layers\n","        out = self.conv1_layer(x)\n","        out = self.drop1(self.conv2_layer(out))\n","        out = self.drop2(self.conv3_layer(out))\n","        #out = self.conv4_layer(out)\n","\n","        # flatten befrore input to linear layer\n","        out = self.flatten(out)\n","\n","        # linear hidden layers\n","        out = F.relu(self.fc1(out))\n","        #out = self.drop1(out)\n","        #out = F.relu(self.fc2(out))\n","        #out = self.drop2(out)\n","\n","        # output layer - no softmax as it is applied by nn.CrossEntropyLoss\n","\n","        out = self.fc2(out)\n","        \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jg9UH-rI8BCV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNMklOPdTunl","executionInfo":{"status":"ok","timestamp":1636953460573,"user_tz":360,"elapsed":43,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"eff26719-22e6-485c-c65c-51fb6c131e9a"},"source":["summary(CIFAR10CNN().cuda(), (3,32,32))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 32, 32]             896\n","              ReLU-2           [-1, 32, 32, 32]               0\n","       BatchNorm2d-3           [-1, 32, 32, 32]              64\n","            Conv2d-4           [-1, 32, 32, 32]           9,248\n","              ReLU-5           [-1, 32, 32, 32]               0\n","         MaxPool2d-6           [-1, 32, 16, 16]               0\n","            Conv2d-7           [-1, 64, 16, 16]          18,496\n","              ReLU-8           [-1, 64, 16, 16]               0\n","       BatchNorm2d-9           [-1, 64, 16, 16]             128\n","           Conv2d-10           [-1, 64, 16, 16]          36,928\n","             ReLU-11           [-1, 64, 16, 16]               0\n","        MaxPool2d-12             [-1, 64, 8, 8]               0\n","          Dropout-13             [-1, 64, 8, 8]               0\n","           Conv2d-14            [-1, 128, 8, 8]          73,856\n","             ReLU-15            [-1, 128, 8, 8]               0\n","      BatchNorm2d-16            [-1, 128, 8, 8]             256\n","           Conv2d-17            [-1, 128, 8, 8]         147,584\n","             ReLU-18            [-1, 128, 8, 8]               0\n","        MaxPool2d-19            [-1, 128, 4, 4]               0\n","          Dropout-20            [-1, 128, 4, 4]               0\n","          Flatten-21                 [-1, 2048]               0\n","           Linear-22                  [-1, 128]         262,272\n","           Linear-23                   [-1, 10]           1,290\n","================================================================\n","Total params: 551,018\n","Trainable params: 551,018\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.36\n","Params size (MB): 2.10\n","Estimated Total Size (MB): 4.47\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ry-7lHYVPlPP"},"source":["# Training Functions"]},{"cell_type":"markdown","metadata":{"id":"z3GdVoTjh7YD"},"source":["## Training Epoch "]},{"cell_type":"code","metadata":{"id":"Pv4x22lZMn5p"},"source":["def train(train_loader, model, optimizer, loss_function, log_batch, log_interval, grad_clipping, max_norm):\n","\n","  \"\"\" \n","  Function for training the model in each epoch\n","  Input: iterator for train dataset, initial weights and bias, epochs, learning rate.\n","  Output: final weights, bias, train loss, train accuracy\n","  \"\"\"\n","  # initilalize variables as global\n","  # these counts will be updated every epoch\n","  global example_ct_train\n","  global batch_ct_train\n","\n","  # Training Loop loop\n","  # Initialize train_loss at the he start of the epoch\n","  running_train_loss = 0\n","  running_train_correct = 0\n","  \n","  # put the model in training mode\n","  model.train()\n","\n","  # Iterate on batches from the dataset using train_loader\n","  for input, targets in train_loader:\n","    \n","    # move inputs and outputs to GPUs\n","    input = input.to(device)\n","    targets = targets.to(device)\n","\n","    # Forward pass\n","    output = model(input)\n","    loss = loss_function(output, targets)\n","\n","    # Correct prediction\n","    y_pred = torch.argmax(output, dim = 1)\n","    correct = torch.sum(y_pred == targets)\n","\n","    example_ct_train +=  len(targets)\n","    batch_ct_train += 1\n","\n","    # set gradients to zero \n","    optimizer.zero_grad()\n","\n","    # Backward pass\n","    loss.backward()\n","\n","    # Gradient Clipping\n","    if grad_clipping:\n","      nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm, norm_type=2)\n","\n","    # Update parameters using their gradient\n","    optimizer.step()\n","          \n","    # Add train loss of a batch \n","    running_train_loss += loss.item()\n","\n","    # Add Corect counts of a batch\n","    running_train_correct += correct\n","\n","    # log batch loss and accuracy\n","    if log_batch:\n","      if ((batch_ct_train + 1) % log_interval) == 0:\n","        wandb.log({f\"Train Batch Loss  :\": loss})\n","        wandb.log({f\"Train Batch Acc :\": correct/len(targets)})\n","\n","    #scheduler.step()\n","  # Calculate mean train loss for the whole dataset for a particular epoch\n","  train_loss = running_train_loss/len(train_loader)\n","\n","\n","\n","  # Calculate accuracy for the whole dataset for a particular epoch\n","  train_acc = running_train_correct/len(train_loader.dataset)\n","\n","  return train_loss, train_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aic8wk5niFCz"},"source":["## Validation/Test Epoch"]},{"cell_type":"code","metadata":{"id":"pHP1WKDessiI"},"source":["def valid(loader, model, optimizer, loss_function, log_batch, log_interval):\n","\n","  \"\"\" \n","  Function for training the model and plotting the graph for train & valid loss vs epoch.\n","  Input: iterator for train dataset, initial weights and bias, epochs, learning rate, batch size.\n","  Output: final weights, bias and train loss and valid loss for each epoch.\n","  \"\"\"\n","\n","  # initilalize variables as global\n","  # these counts will be updated every epoch\n","  global example_ct_valid\n","  global batch_ct_valid\n","\n","  # Validation loop\n","  # Initialize train_loss at the he strat of the epoch\n","  running_valid_loss = 0\n","  running_valid_correct = 0\n","  \n","  # put the model in evaluation mode\n","  model.eval()\n","\n","  with torch.no_grad():\n","    for input,targets in loader:\n","\n","      # move inputs and outputs to GPUs\n","      input = input.to(device)\n","      targets = targets.to(device)\n","\n","      # Forward pass\n","      output = model(input)\n","      loss = loss_function(output,targets)\n","\n","      # Correct Predictions\n","      y_pred = torch.argmax(output, dim = 1)\n","      correct = torch.sum(y_pred == targets)\n","\n","      # count of images and batches\n","      example_ct_valid +=  len(targets)\n","      batch_ct_valid += 1\n","\n","      # Add valid loss of a batch \n","      running_valid_loss += loss.item()\n","\n","      # Add correct count for each batch\n","      running_valid_correct += correct\n","\n","      # log batch loss and accuracy\n","      if log_batch:\n","        if ((batch_ct_valid + 1) % log_interval) == 0:\n","          wandb.log({f\"Valid Batch Loss  :\": loss})\n","          wandb.log({f\"Valid Batch Accuracy :\": correct/len(targets)})\n","\n","\n","    # Calculate mean valid loss for the whole dataset for a particular epoch\n","    valid_loss = running_valid_loss/len(valid_loader)\n","\n","    # scheduler step\n","    #scheduler.step(valid_loss)\n","    # scheduler.step()\n","\n","    # Calculate accuracy for the whole dataset for a particular epoch\n","    valid_acc = running_valid_correct/len(valid_loader.dataset)\n","    \n","  return valid_loss, valid_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwF70eqE6n_v"},"source":["##  Model Training Loop\n"]},{"cell_type":"code","metadata":{"id":"KeCKVgg-5FiZ"},"source":["def train_loop(train_loader, valid_loader, model, loss_function, optimizer, epochs, device, patience, early_stopping,\n","               file_model):\n","\n","  '''\n","  model: specify your model for training\n","  criterion: loss function \n","  optimizer: optimizer like SGD , ADAM etc.\n","  train loader: function to carete batches for training data\n","  loader : function to create batches for valid data set\n","  file_model : specify file name for saving your model. This way we can upload the model weights from file. We will not to run model again.\n","  \n","\n","  '''\n","  # Create lists to store train and valid loss at each epoch\n","\n","  train_loss_history = []\n","  valid_loss_history = []\n","  train_acc_history = []\n","  valid_acc_history = []\n","  delta = 0\n","  best_score = None\n","  valid_loss_min = np.Inf\n","  counter_early_stop=0\n","  early_stop=False\n","\n","\n","  # Iterate for the given number of epochs\n","  for epoch in range(epochs):\n","    t0 = datetime.now()\n","    # Get train loss and accuracy for one epoch\n","\n","    train_loss, train_acc = train(train_loader, model, optimizer, loss_function, \n","                                  wandb.config.log_batch, wandb.config.log_interval,\n","                                  wandb.config.grad_clipping, wandb.config.max_norm)\n","    valid_loss, valid_acc = valid(valid_loader, model, optimizer, loss_function,\n","                                    wandb.config.log_batch, wandb.config.log_interval)\n","\n","    dt = datetime.now() - t0\n","\n","    # Save history of the Losses and accuracy\n","    train_loss_history.append(train_loss)\n","    train_acc_history.append(train_acc)\n","    valid_loss_history.append(valid_loss)\n","    valid_acc_history.append(valid_acc)\n","\n","    if early_stopping:\n","      score = -valid_loss\n","      if best_score is None:\n","        best_score=score\n","        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n","        torch.save(model.state_dict(), file_model)\n","        valid_loss_min = valid_loss\n","\n","      elif score < best_score + delta:\n","        counter_early_stop += 1\n","        print(f'Early stoping counter: {counter_early_stop} out of {patience}')\n","        if counter_early_stop > patience:\n","          early_stop = True\n","\n","      \n","      else:\n","        best_score = score\n","        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n","        torch.save(model.state_dict(), file_model)\n","        counter_early_stop=0\n","        valid_loss_min = valid_loss\n","\n","      if early_stop:\n","        print('Early Stopping')\n","        break\n","\n","    else:\n","\n","      score = -valid_loss\n","      if best_score is None:\n","        best_score=score\n","        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n","        torch.save(model.state_dict(), file_model)\n","        valid_loss_min = valid_loss\n","\n","      elif score < best_score + delta:\n","        print(f'Validation loss has not decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Not Saving Model...')\n","      \n","      else:\n","        best_score = score\n","        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n","        torch.save(model.state_dict(), file_model)\n","        valid_loss_min = valid_loss\n","\n","\n","\n","    # Log the train and valid loss to W&B\n","    wandb.log({f\"Train epoch Loss :\": train_loss, f\"Valid epoch Loss :\": valid_loss })\n","    wandb.log({f\"Train epoch Acc :\": train_acc, f\"Valid epoch Acc :\": valid_acc})\n","\n","\n","\n","    # Print the train loss and accuracy for given number of epochs, batch size and number of samples\n","    print(f'Epoch : {epoch+1} / {epochs}')\n","    print(f'Time to complete {epoch+1} is {dt}')\n","    # print(f'Learning rate: {scheduler.get_last_lr()}')\n","    #print(f'Learning rate: {scheduler._last_lr[0]}')\n","    print(f'Train Loss: {train_loss : .4f} | Train Accuracy: {train_acc * 100 : .4f}%')\n","    print(f'Valid Loss: {valid_loss : .4f} | Valid Accuracy: {valid_acc * 100 : .4f}%')\n","    print()\n","    torch.cuda.empty_cache()\n","\n","  return train_loss_history, train_acc_history, valid_loss_history, valid_acc_history\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yte1HzNniWlr"},"source":["#Model Training"]},{"cell_type":"markdown","metadata":{"id":"8fLa11d5rPZv"},"source":["## **Meta data**\n","\n"]},{"cell_type":"code","metadata":{"id":"F3ISekWSV84a"},"source":["hyperparameters = dict(\n","    epochs = 10,\n","    output_dim = 10, \n","    \n","    batch_size = 64,\n","    learning_rate = 0.01,\n","    dataset=\"CIFAR10\",\n","    architecture=\"CNN\",\n","    log_interval = 25,\n","    log_batch = True,\n","    file_model = lecture_folder/'CIFAR10_CNN_experiment_48.pt',\n","    grad_clipping = True,\n","    max_norm = 1,\n","    patience = 5,\n","    early_stopping = True,\n","    weight_decay = 0,\n","    scheduler_factor = 0,\n","    scheduler_patience = 0,\n","   )\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xH25I8Pikqpb"},"source":["## Initialize wandb"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":692,"referenced_widgets":["01751c0758e14356bd4cbe22a9a5c026","9748374ee0d647e395bcb0ac5f3d5fdd","a26feabb3c444cf6b03472e37fcd83aa","9eda2dd8c007486a84648254d17e1d4a","14a37670d01142af873f59e3058bbdca","1bebf1bc695f440490f86b4b15e63261","c5cfdaea6d9246748086d5af7b3f9303","4045ebcbe4b6427d879b9453568260b6"]},"id":"9Zp9fDrAheXc","executionInfo":{"status":"ok","timestamp":1636953469620,"user_tz":360,"elapsed":9074,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"6b8ee74c-fa0d-4525-dc88-9c284b2c00d3"},"source":["wandb.init(name = 'CIFAR10CNN_hw5_exp_48', project = 'CIFAR10_CNN', config = hyperparameters)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["Finishing last run (ID:2u44ovmn) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["<br/>Waiting for W&B process to finish, PID 12140... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01751c0758e14356bd4cbe22a9a5c026","version_minor":0,"version_major":2},"text/plain":["VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Batch Acc :</td><td>▁▂▁▁▃▃▄▅▅▅▆█▅▇▅▇▆▆▇▆█▇▆█▇▆▇▇█▇█████████▇</td></tr><tr><td>Train Batch Loss  :</td><td>██▇▇▅▆▆▅▅▃▃▃▄▄▄▂▃▃▂▃▂▂▃▂▂▄▃▃▁▂▂▂▁▁▂▂▁▁▂▂</td></tr><tr><td>Train epoch Acc :</td><td>▁▃▅▆▆▇▇███</td></tr><tr><td>Train epoch Loss :</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Valid Batch Accuracy :</td><td>▂▂▂▁▆▂▃▃▆▆▅▃▃▅▆▅█▆▇▇▇▇█▇█▅▆▇▇▇▆█▇▇▇▇▇▆▆▇</td></tr><tr><td>Valid Batch Loss  :</td><td>█▇▇▇▆▇▇▆▅▃▅▆▆▄▃▅▂▄▂▂▂▃▂▂▃▃▄▃▂▃▂▂▄▂▁▃▂▃▄▂</td></tr><tr><td>Valid epoch Acc :</td><td>▁▄▅▆▇▇▇█▇█</td></tr><tr><td>Valid epoch Loss :</td><td>█▆▄▄▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Batch Acc :</td><td>0.625</td></tr><tr><td>Train Batch Loss  :</td><td>0.9916</td></tr><tr><td>Train epoch Acc :</td><td>0.70307</td></tr><tr><td>Train epoch Loss :</td><td>0.88324</td></tr><tr><td>Valid Batch Accuracy :</td><td>0.71875</td></tr><tr><td>Valid Batch Loss  :</td><td>0.80992</td></tr><tr><td>Valid epoch Acc :</td><td>0.7029</td></tr><tr><td>Valid epoch Loss :</td><td>0.85269</td></tr></table>\n","</div></div>\n","Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">CIFAR10CNN_hw5_exp_47</strong>: <a href=\"https://wandb.ai/sxs200126/CIFAR10_CNN/runs/2u44ovmn\" target=\"_blank\">https://wandb.ai/sxs200126/CIFAR10_CNN/runs/2u44ovmn</a><br/>\n","Find logs at: <code>./wandb/run-20211115_051150-2u44ovmn/logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Successfully finished last run (ID:2u44ovmn). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/sxs200126/CIFAR10_CNN/runs/wgkao01n\" target=\"_blank\">CIFAR10CNN_hw5_exp_48</a></strong> to <a href=\"https://wandb.ai/sxs200126/CIFAR10_CNN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f6f11098ed0>"],"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sxs200126/CIFAR10_CNN/runs/wgkao01n?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"]},"metadata":{},"execution_count":1118}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GUCW6ipgS8p","executionInfo":{"status":"ok","timestamp":1636953469623,"user_tz":360,"elapsed":73,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"29704197-08fb-45ca-9519-2261d3ba8d5d"},"source":["wandb.config.device = device\n","print(wandb.config.device )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"markdown","metadata":{"id":"B9DjhWa3k-Gk"},"source":["## Specify Dataloader, Loss_function, Model, Optimizer, Weight Initialization"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-10-30T03:59:34.389768Z","iopub.status.busy":"2021-10-30T03:59:34.389603Z","iopub.status.idle":"2021-10-30T03:59:34.435800Z","shell.execute_reply":"2021-10-30T03:59:34.435486Z","shell.execute_reply.started":"2021-10-30T03:59:34.389752Z"},"tags":[],"id":"z7zBv-QN-quA"},"source":["# Fix seed value\n","SEED = 2345\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","# Data Loader\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=wandb.config.batch_size, shuffle = True)\n","valid_loader = torch.utils.data.DataLoader(validset, batch_size=wandb.config.batch_size, shuffle = False)\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=wandb.config.batch_size,   shuffle = False)\n","\n","# cross entropy loss function\n","loss_function = nn.CrossEntropyLoss()\n","\n","# device \n","model = CIFAR10CNN()\n","\n","def init_weights(m):\n","  if type(m) == nn.Conv2d:\n","        torch.nn.init.kaiming_normal_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n","\n","  if type(m) == nn.Conv2d:\n","        torch.nn.init.kaiming_normal_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n","\n","        \n","# apply initialization recursively  to all modules\n","# model.apply(init_weights)\n","\n","wandb.config.init_weights = init_weights\n","\n","# put model to GPUs\n","model.to(wandb.config.device)\n","\n","# Intialize stochiastic gradient descent optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr = wandb.config.learning_rate, weight_decay=wandb.config.weight_decay, momentum = 0.9)\n","#optimizer = torch.optim.Adam(model.parameters(), lr = wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n","\n","wandb.config.optimizer = optimizer\n","\n","#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor= wandb.config.scheduler_factor, \n"," #                             patience=wandb.config.scheduler_patience, verbose=True)\n","\n","#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, total_steps=len(train_loader) * 10 , epochs=10, three_phase=True)\n","\n","#scheduler = StepLR(optimizer, gamma=0.4,step_size=1, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"htWT1-Nhlw3m"},"source":["## Train Model and Save best model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwnGtxo3B2nD","executionInfo":{"status":"ok","timestamp":1636953469628,"user_tz":360,"elapsed":59,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"4a139d33-092b-4c18-dfd9-da79fd011b70"},"source":["wandb.watch(model, log = 'all', log_freq=25, log_graph=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"]},{"output_type":"execute_result","data":{"text/plain":["[<wandb.wandb_torch.TorchGraph at 0x7f6f1131eb50>]"]},"metadata":{},"execution_count":1121}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyAaOEg5DdKp","executionInfo":{"status":"ok","timestamp":1636953677584,"user_tz":360,"elapsed":207993,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"85910a3d-c6f2-492e-e6d7-0698c148db2f"},"source":["example_ct_train, batch_ct_train, example_ct_valid, batch_ct_valid = 0, 0, 0, 0\n","train_loss_history, train_acc_history, valid_loss_history, valid_acc_history = train_loop(train_loader, valid_loader, model, loss_function, optimizer, \n","                                                                                          wandb.config.epochs, wandb.config.device,\n","                                                                                          wandb.config.patience, wandb.config.early_stopping,\n","                                                                                          wandb.config.file_model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss has decreased (inf --> 1.166863). Saving Model...\n","Epoch : 1 / 10\n","Time to complete 1 is 0:00:20.788466\n","Train Loss:  1.4333 | Train Accuracy:  47.2575%\n","Valid Loss:  1.1669 | Valid Accuracy:  59.2100%\n","\n","Validation loss has decreased (1.166863 --> 0.895109). Saving model...\n","Epoch : 2 / 10\n","Time to complete 2 is 0:00:20.770825\n","Train Loss:  0.9970 | Train Accuracy:  64.3350%\n","Valid Loss:  0.8951 | Valid Accuracy:  68.0700%\n","\n","Validation loss has decreased (0.895109 --> 0.798625). Saving model...\n","Epoch : 3 / 10\n","Time to complete 3 is 0:00:20.823256\n","Train Loss:  0.8336 | Train Accuracy:  70.7275%\n","Valid Loss:  0.7986 | Valid Accuracy:  71.8300%\n","\n","Validation loss has decreased (0.798625 --> 0.711801). Saving model...\n","Epoch : 4 / 10\n","Time to complete 4 is 0:00:20.808625\n","Train Loss:  0.7352 | Train Accuracy:  73.8000%\n","Valid Loss:  0.7118 | Valid Accuracy:  74.9000%\n","\n","Validation loss has decreased (0.711801 --> 0.648964). Saving model...\n","Epoch : 5 / 10\n","Time to complete 5 is 0:00:20.719397\n","Train Loss:  0.6572 | Train Accuracy:  76.7825%\n","Valid Loss:  0.6490 | Valid Accuracy:  76.8800%\n","\n","Validation loss has decreased (0.648964 --> 0.642340). Saving model...\n","Epoch : 6 / 10\n","Time to complete 6 is 0:00:20.813878\n","Train Loss:  0.5992 | Train Accuracy:  79.0650%\n","Valid Loss:  0.6423 | Valid Accuracy:  77.2500%\n","\n","Validation loss has decreased (0.642340 --> 0.595776). Saving model...\n","Epoch : 7 / 10\n","Time to complete 7 is 0:00:20.712375\n","Train Loss:  0.5524 | Train Accuracy:  80.7150%\n","Valid Loss:  0.5958 | Valid Accuracy:  78.6700%\n","\n","Early stoping counter: 1 out of 5\n","Epoch : 8 / 10\n","Time to complete 8 is 0:00:20.769368\n","Train Loss:  0.5088 | Train Accuracy:  82.2475%\n","Valid Loss:  0.6188 | Valid Accuracy:  78.4100%\n","\n","Validation loss has decreased (0.595776 --> 0.564675). Saving model...\n","Epoch : 9 / 10\n","Time to complete 9 is 0:00:20.649764\n","Train Loss:  0.4754 | Train Accuracy:  83.3050%\n","Valid Loss:  0.5647 | Valid Accuracy:  80.2500%\n","\n","Validation loss has decreased (0.564675 --> 0.555136). Saving model...\n","Epoch : 10 / 10\n","Time to complete 10 is 0:00:20.674604\n","Train Loss:  0.4387 | Train Accuracy:  84.5075%\n","Valid Loss:  0.5551 | Valid Accuracy:  80.5600%\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"FWCLH47azD6j"},"source":["# **Accuracy and Predictions**\n","\n","Now we have final values for weights and bias after training the model. We will use these values to make predictions on the test dataset."]},{"cell_type":"markdown","metadata":{"id":"uzDX4D6EvcVC"},"source":["## Function to get predictions"]},{"cell_type":"code","metadata":{"id":"M6KZqsnqQFVu"},"source":["def get_acc_pred(data_loader, model):\n","  \"\"\" \n","  Function to get predictions for a given test set and calculate accuracy.\n","  Input: Iterator to the test set.\n","  Output: Prections and Accuracy for test set.\n","  \"\"\"\n","  with torch.no_grad():\n","    # Array to store predicted labels\n","    predictions = torch.Tensor()\n","    predictions = predictions.to(device)\n","\n","    # Array to store actual labels\n","    y = torch.Tensor()\n","    y = y.to(device)\n","    # Iterate over batches from test set\n","    for input, targets in data_loader:\n","      \n","      # move inputs and outputs to GPUs\n","      input = input.to(device)\n","      targets = targets.to(device)\n","\n","      # Calculated the predicted labels\n","      output = model(input)\n","\n","      # Choose the label with maximum probability\n","      indices = torch.argmax(output, dim = 1)\n","\n","      # Add the predicted labels to the array\n","      predictions = torch.cat((predictions, indices)) \n","\n","      # Add the actual labels to the array\n","      y = torch.cat((y, targets)) \n","\n","    # Check for complete dataset if actual and predicted labels are same or not\n","    # Calculate accuracy\n","    acc = (predictions == y).float().mean()\n","\n","  # Return array containing predictions and accuracy\n","  return predictions, acc\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lK-g-bSTvkl8"},"source":["## Load saved model from file "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVT_qlEivpER","executionInfo":{"status":"ok","timestamp":1636953677594,"user_tz":360,"elapsed":49,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"93f562c4-4444-40fc-9deb-9cce16edf326"},"source":["model_CNN =  CIFAR10CNN()\n","model_CNN.to(device)\n","model_CNN.load_state_dict(torch.load(wandb.config.file_model))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":1124}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKNCkt_XeCZ1","executionInfo":{"status":"ok","timestamp":1636953677598,"user_tz":360,"elapsed":40,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"cb52f59f-1d0c-431d-ec9a-d9fd384077c7"},"source":["print(wandb.config.file_model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Deep Learning/HW5/CIFAR10_CNN_experiment_48.pt\n"]}]},{"cell_type":"code","metadata":{"id":"6mZ5tlp53hi7"},"source":["# Get the prediction and accuracy for the test dataset\n","predictions, acc_test = get_acc_pred(test_loader, model_CNN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3v2z0oFcRjrF","executionInfo":{"status":"ok","timestamp":1636953680588,"user_tz":360,"elapsed":44,"user":{"displayName":"Shishir sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQXaVDw08iapj6wCi7UM1wKF5TLN-xVAZbzWZzDA=s64","userId":"18124275142639548991"}},"outputId":"67ccf83f-c134-4475-d997-abb38f4a730d"},"source":["# Print Accuracy for test dataset\n","print(acc_test * 100)\n","wandb.config.test_accuracy = acc_test"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(78.3500, device='cuda:0')\n"]}]}]}